# LLM服务配置
# 此文件定义了系统可用的LLM服务

ollama:
  name: "Ollama"  # 服务名称
  api_base: "http://localhost:11434/v1"  # API基础URL，与Roles.md一致
  api_key: "ollama"  # API密钥，与Roles.md一致
  models:  # 可用的模型列表
    - "qwen2:7b"
  default_model: "qwen2:7b"  # 默认使用的模型，与Roles.md一致
  timeout: 300  # API调用超时时间（秒），设置为5分钟以确保有充分的等待时间
  temperature: 0.7  # 生成文本的温度参数，控制随机性
  max_tokens: 2000  # 最大生成token数